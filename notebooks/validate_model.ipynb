{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGEX Model Validation\n",
    "\n",
    "Post-training validation notebook to verify the finetuned model:\n",
    "\n",
    "1. Load the base model + LoRA adapter\n",
    "2. Test generation on sample prompts\n",
    "3. Check output format compliance (COGEX structure)\n",
    "4. Compute validation perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "load_dotenv()\n",
    "MODELS_PATH = os.getenv('MODELS_PATH')\n",
    "print(f\"MODELS_PATH: {MODELS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Update these paths to point to your trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURE THESE\n",
    "BASE_MODEL = \"Llama-3.1-8B\"  # Base model name\n",
    "ADAPTER_PATH = \"outputs/cogex-Llama-3.1-8B-YYYYMMDD_HHMMSS\"  # Path to trained adapter\n",
    "\n",
    "# Paths\n",
    "BASE_MODEL_PATH = Path(MODELS_PATH) / BASE_MODEL\n",
    "VAL_DATA_PATH = \"data/val.json\"\n",
    "\n",
    "print(f\"Base model: {BASE_MODEL_PATH}\")\n",
    "print(f\"Adapter: {ADAPTER_PATH}\")\n",
    "print(f\"\\nBase model exists: {BASE_MODEL_PATH.exists()}\")\n",
    "print(f\"Adapter exists: {Path(ADAPTER_PATH).exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading base model...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_PATH,\n",
    "    local_files_only=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    BASE_MODEL_PATH,\n",
    "    local_files_only=True,\n",
    ")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Loading LoRA adapter...\")\n",
    "model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)\n",
    "model.eval()\n",
    "\n",
    "print(f\"\\nModel loaded successfully!\")\n",
    "print(f\"Device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cogex_output(\n",
    "    instruction: str,\n",
    "    input_text: str = \"\",\n",
    "    max_new_tokens: int = 512,\n",
    "    temperature: float = 0.7,\n",
    "    do_sample: bool = True,\n",
    ") -> str:\n",
    "    \"\"\"Generate COGEX-style code and execution output.\"\"\"\n",
    "    # Format prompt (same as training)\n",
    "    prompt = f\"### Instruction\\n{instruction}\\n### Input\\n{input_text}\\n\"\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=do_sample,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases\n",
    "test_cases = [\n",
    "    {\n",
    "        \"instruction\": \"Give three tips for staying healthy.\",\n",
    "        \"input\": \"\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Identify the odd one out.\",\n",
    "        \"input\": '[\"Twitter\", \"Instagram\", \"Telegram\"]'\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Pluralize the given word.\",\n",
    "        \"input\": \"Corpus\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What are the three primary colors?\",\n",
    "        \"input\": \"\"\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run tests\n",
    "for i, case in enumerate(test_cases):\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Test Case {i+1}\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Instruction: {case['instruction']}\")\n",
    "    print(f\"Input: {case['input'] or '(empty)'}\")\n",
    "    print(\"\\n--- Generated Output ---\\n\")\n",
    "    \n",
    "    output = generate_cogex_output(case['instruction'], case['input'])\n",
    "    print(output)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Format Compliance Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_output_format(output: str) -> dict:\n",
    "    \"\"\"Check if generated output follows COGEX format.\"\"\"\n",
    "    checks = {\n",
    "        'has_code_section': '### Code' in output,\n",
    "        'has_output_section': '### Output' in output,\n",
    "        'has_function_def': 'def ' in output,\n",
    "        'has_function_call': '>>>' in output,\n",
    "        'has_return_dict': \"'answer'\" in output or '\"answer\"' in output,\n",
    "        'has_docstring': '\"\"\"' in output,\n",
    "        'has_undefined_functions': False,  # Check for calls without def\n",
    "    }\n",
    "    \n",
    "    # Check for undefined function calls (pseudo-code pattern)\n",
    "    import re\n",
    "    # Look for function calls that aren't defined in the output\n",
    "    func_calls = re.findall(r'= (\\w+)\\(', output)\n",
    "    func_defs = re.findall(r'def (\\w+)\\(', output)\n",
    "    undefined = set(func_calls) - set(func_defs) - {'print', 'len', 'str', 'int', 'list', 'dict', 'range'}\n",
    "    checks['has_undefined_functions'] = len(undefined) > 0\n",
    "    checks['undefined_functions'] = list(undefined)\n",
    "    \n",
    "    checks['all_passed'] = all([\n",
    "        checks['has_code_section'],\n",
    "        checks['has_output_section'],\n",
    "        checks['has_function_def'],\n",
    "        checks['has_function_call'],\n",
    "        checks['has_return_dict'],\n",
    "    ])\n",
    "    \n",
    "    return checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run format compliance on test cases\n",
    "print(\"Format Compliance Results\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "format_results = []\n",
    "for i, case in enumerate(test_cases):\n",
    "    output = generate_cogex_output(case['instruction'], case['input'])\n",
    "    result = check_output_format(output)\n",
    "    format_results.append(result)\n",
    "    \n",
    "    status = \"PASS\" if result['all_passed'] else \"FAIL\"\n",
    "    print(f\"\\nTest {i+1}: {status}\")\n",
    "    print(f\"  Instruction: {case['instruction'][:50]}...\")\n",
    "    for key, value in result.items():\n",
    "        if key not in ['all_passed', 'undefined_functions']:\n",
    "            symbol = \"✓\" if value else \"✗\"\n",
    "            print(f\"  {symbol} {key}\")\n",
    "    if result['undefined_functions']:\n",
    "        print(f\"  Undefined functions: {result['undefined_functions']}\")\n",
    "\n",
    "# Summary\n",
    "n_passed = sum(1 for r in format_results if r['all_passed'])\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(f\"Format compliance: {n_passed}/{len(format_results)} passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Validation Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load validation data\n",
    "print(f\"Loading validation data from {VAL_DATA_PATH}...\")\n",
    "with open(VAL_DATA_PATH, 'r') as f:\n",
    "    val_data = json.load(f)\n",
    "print(f\"Loaded {len(val_data)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(examples: list, max_examples: int = 100) -> float:\n",
    "    \"\"\"Compute perplexity on validation examples.\"\"\"\n",
    "    import math\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    for i, ex in enumerate(examples[:max_examples]):\n",
    "        if i % 20 == 0:\n",
    "            print(f\"  Processing {i}/{min(len(examples), max_examples)}...\")\n",
    "        \n",
    "        text = ex['text']\n",
    "        inputs = tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=2048\n",
    "        )\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, labels=inputs['input_ids'])\n",
    "            loss = outputs.loss.item()\n",
    "        \n",
    "        n_tokens = inputs['input_ids'].shape[1]\n",
    "        total_loss += loss * n_tokens\n",
    "        total_tokens += n_tokens\n",
    "    \n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    \n",
    "    return perplexity, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Computing validation perplexity (first 100 examples)...\")\n",
    "ppl, avg_loss = compute_perplexity(val_data, max_examples=100)\n",
    "\n",
    "print(f\"\\n{'=' * 50}\")\n",
    "print(\"Validation Metrics\")\n",
    "print(f\"{'=' * 50}\")\n",
    "print(f\"Average Loss: {avg_loss:.4f}\")\n",
    "print(f\"Perplexity: {ppl:.2f}\")\n",
    "print(f\"{'=' * 50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare with Base Model (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to compare with base model (without LoRA)\n",
    "# This helps verify the finetuning improved performance\n",
    "\n",
    "# print(\"\\nLoading base model WITHOUT adapter for comparison...\")\n",
    "# base_only = AutoModelForCausalLM.from_pretrained(\n",
    "#     BASE_MODEL_PATH,\n",
    "#     local_files_only=True,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "# base_only.eval()\n",
    "\n",
    "# # Temporarily swap model for perplexity computation\n",
    "# _model_backup = model\n",
    "# model = base_only\n",
    "\n",
    "# print(\"Computing base model perplexity...\")\n",
    "# base_ppl, base_loss = compute_perplexity(val_data, max_examples=100)\n",
    "\n",
    "# model = _model_backup\n",
    "# del base_only\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# print(f\"\\nComparison:\")\n",
    "# print(f\"  Base model PPL: {base_ppl:.2f}\")\n",
    "# print(f\"  Finetuned PPL:  {ppl:.2f}\")\n",
    "# print(f\"  Improvement:    {(base_ppl - ppl) / base_ppl * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"VALIDATION SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nModel: {BASE_MODEL}\")\n",
    "print(f\"Adapter: {ADAPTER_PATH}\")\n",
    "print(f\"\\nMetrics:\")\n",
    "print(f\"  Validation Perplexity: {ppl:.2f}\")\n",
    "print(f\"  Format Compliance: {n_passed}/{len(format_results)} passed\")\n",
    "print(f\"\\nChecklist:\")\n",
    "print(f\"  [{'✓' if ppl < 50 else '?'}] Perplexity reasonable (< 50 expected for finetuned model)\")\n",
    "print(f\"  [{'✓' if n_passed == len(format_results) else '✗'}] All format checks pass\")\n",
    "print(f\"  [ ] Manual review of generated outputs looks correct\")\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
